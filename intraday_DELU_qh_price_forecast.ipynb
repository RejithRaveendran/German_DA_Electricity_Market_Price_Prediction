{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2b5b6ce-aa78-46b8-8070-c0f780a8ece5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "✅ Notebook boot OK.\n",
      "✅ ENTSO-E token detected.\n",
      "✅ Config loaded.\n",
      "ENTSO-E load forecast failed: TypeError(\"'str' object is not callable\")\n",
      "✅ DA feature store saved: ./feature_store/da_features_qh.parquet (shape=(39771, 12))\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1325\n",
      "[LightGBM] [Info] Number of data points in the train set: 33805, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 95.599998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ESC RPA Team\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\Users\\ESC RPA Team\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "C:\\Users\\ESC RPA Team\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "C:\\Users\\ESC RPA Team\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\Users\\ESC RPA Team\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "C:\\Users\\ESC RPA Team\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001500 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1325\n",
      "[LightGBM] [Info] Number of data points in the train set: 33805, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 3.900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ESC RPA Team\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\Users\\ESC RPA Team\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "C:\\Users\\ESC RPA Team\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002355 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1325\n",
      "[LightGBM] [Info] Number of data points in the train set: 33805, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 144.610001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ESC RPA Team\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ESC RPA Team\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ESC RPA Team\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE (p50): 8.869997884428805\n",
      "Pinball@10: 3.181057764373718 Pinball@50: 4.434998942214403 Pinball@90: 3.5834322034595587\n",
      "✅ DA models saved.\n",
      "ENTSO-E load forecast failed: NoMatchingDataError()\n",
      "ENTSO-E wind/solar forecast failed: NoMatchingDataError()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ESC RPA Team\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ESC RPA Team\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ESC RPA Team\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved DA forecast CSV (tomorrow): ./artifacts/DA_forecast_DE-LU_2025-11-25.csv (rows=97)\n",
      "ENTSO-E load forecast failed: TypeError(\"'str' object is not callable\")\n",
      "✅ Prev-day metrics → MAE: 19.053 EUR/MWh | RMSE: 21.944 EUR/MWh | Coverage (p10–p90): 9.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ESC RPA Team\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ESC RPA Team\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ESC RPA Team\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# === ONE-CELL NOTEBOOK: DE-LU Day-Ahead (DA) 15-min Price Forecast → two CSVs ===\n",
    "%pip install -q pandas numpy requests pytz python-dateutil scikit-learn lightgbm matplotlib seaborn pyarrow joblib entsoe-py\n",
    "\n",
    "import os, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import timedelta, date\n",
    "from time import sleep\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "print(\"✅ Notebook boot OK.\")\n",
    "\n",
    "# -------------------------- Configuration --------------------------\n",
    "# ENTSO-E token (Transparency Platform REST API).\n",
    "# Ref: entsoe-py usage & tz requirements: https://pypi.org/project/entsoe-py/  [1](https://github.com/nager/Nager.Date)\n",
    "os.environ[\"ENTSOE_API_TOKEN\"] = \"c75a0111-6841-4300-a9d8-e3746d32ac5e\"  # <-- replace if needed\n",
    "ENTSOE_TOKEN = os.getenv(\"ENTSOE_API_TOKEN\") or os.getenv(\"ENTSOE_API\") or \"\"\n",
    "print(\"✅ ENTSO-E token detected.\" if ENTSOE_TOKEN else \"⚠️ ENTSO-E token NOT detected\")\n",
    "\n",
    "ZONE_CODE_ENTSOE = \"DE_LU\"     # Germany-Luxembourg bidding zone in entsoe-py  [1](https://github.com/nager/Nager.Date)\n",
    "TZ_LOCAL   = \"Europe/Berlin\"   # local output tz (DST-safe)\n",
    "TZ_ENTSOE  = \"Europe/Brussels\" # entsoe-py expects Brussels tz  [1](https://github.com/nager/Nager.Date)\n",
    "\n",
    "FEATURE_STORE_DIR = \"./feature_store\"; os.makedirs(FEATURE_STORE_DIR, exist_ok=True)\n",
    "MODEL_DIR         = \"./models\";       os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "ARTIFACTS_DIR     = \"./artifacts\";    os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "# Training window (spans DA 15-min from 2025-10-01 onward; normalize earlier data to 15-min)\n",
    "TRAIN_START = pd.Timestamp(\"2024-10-01\", tz=TZ_LOCAL)\n",
    "TRAIN_END   = pd.Timestamp.today(tz=TZ_LOCAL) - pd.Timedelta(days=5)\n",
    "\n",
    "# Forecast days (local)\n",
    "TOMORROW_LOCAL    = (pd.Timestamp.today(tz=TZ_LOCAL) + pd.Timedelta(days=1)).normalize()\n",
    "TODAY_LOCAL       = pd.Timestamp.today(tz=TZ_LOCAL).normalize()\n",
    "PREV_LOCAL_DATE   = TODAY_LOCAL - pd.Timedelta(days=1)   # for accuracy check\n",
    "\n",
    "# Fallback open sources:\n",
    "SMARD_BASE         = \"https://www.smard.de/app/chart_data\"   # DA 15-min since Oct 1, 2025  [2](https://www.entsoe.eu/network_codes/bzr/)\n",
    "ENERGY_CHARTS_BASE = \"https://api.energy-charts.info\"        # public API (no token)        [3](http://www.world-timedate.com/dst/daylight_saving_time.php?city_id=162)\n",
    "\n",
    "print(\"✅ Config loaded.\")\n",
    "\n",
    "# -------------------------- Helpers --------------------------\n",
    "def make_qh_index(start_ts, end_ts, tzname=TZ_LOCAL):\n",
    "    \"\"\"DST-safe quarter-hour index.\"\"\"\n",
    "    return pd.date_range(start=start_ts, end=end_ts, freq=\"15min\", tz=tzname)\n",
    "\n",
    "def to_brussels(ts):\n",
    "    \"\"\"Ensure tz-aware Timestamp in Europe/Brussels for entsoe-py.\"\"\"\n",
    "    ts = pd.Timestamp(ts)\n",
    "    return ts.tz_localize(TZ_ENTSOE) if ts.tz is None else ts.tz_convert(TZ_ENTSOE)\n",
    "\n",
    "# -------------------------- ENTSO-E client --------------------------\n",
    "from entsoe import EntsoePandasClient\n",
    "pc = EntsoePandasClient(api_key=ENTSOE_TOKEN)\n",
    "\n",
    "# -------------------------- DA series (ENTSO-E → SMARD → Energy-Charts) --------------------------\n",
    "# SMARD fallback: 15-min DA since Oct 1, 2025.  [2](https://www.entsoe.eu/network_codes/bzr/)\n",
    "def smard_index(filter_id, resolution=\"quarterhour\", region=\"DE\"):\n",
    "    r = requests.get(f\"{SMARD_BASE}/{filter_id}/{region}/index_{resolution}.json\", timeout=30)\n",
    "    r.raise_for_status(); return r.json()\n",
    "\n",
    "def smard_series(filter_id, ts_token, resolution=\"quarterhour\", region=\"DE\"):\n",
    "    r = requests.get(f\"{SMARD_BASE}/{filter_id}/{region}/{filter_id}_{region}_{resolution}_{ts_token}.json\", timeout=30)\n",
    "    r.raise_for_status(); return r.json()\n",
    "\n",
    "def get_smard_da_qh():\n",
    "    for fid in [4169, 8004169]:  # community-known ids (may change)\n",
    "        try:\n",
    "            idx = smard_index(fid, \"quarterhour\", \"DE\")\n",
    "            if idx.get(\"timestamps\"):\n",
    "                token = idx[\"timestamps\"][-1]\n",
    "                data = smard_series(fid, token, \"quarterhour\", \"DE\")\n",
    "                df = pd.DataFrame(data.get(\"series\", []), columns=[\"unix_ms\", \"eur_per_mwh\"])\n",
    "                if not df.empty:\n",
    "                    df[\"timestamp\"] = pd.to_datetime(df[\"unix_ms\"], unit=\"ms\", utc=True).dt.tz_convert(TZ_LOCAL)\n",
    "                    return df.set_index(\"timestamp\")[[\"eur_per_mwh\"]].rename(columns={\"eur_per_mwh\":\"da_eur_per_mwh\"}).sort_index()\n",
    "        except Exception: pass\n",
    "    return pd.DataFrame(columns=[\"da_eur_per_mwh\"])\n",
    "\n",
    "# Energy-Charts fallback: public API (no token).  [3](http://www.world-timedate.com/dst/daylight_saving_time.php?city_id=162)\n",
    "def get_energy_charts_da(country=\"DE\", year=None):\n",
    "    if year is None: year = date.today().year\n",
    "    for url in [f\"{ENERGY_CHARTS_BASE}/price_day_ahead?country={country}&year={year}\",\n",
    "                f\"{ENERGY_CHARTS_BASE}/price_spot?country={country}&year={year}\"]:\n",
    "        try:\n",
    "            r = requests.get(url, timeout=30)\n",
    "            if r.ok:\n",
    "                js = r.json(); vals = js.get(\"price\", js.get(\"values\"))\n",
    "                if js.get(\"unix_seconds\") and vals:\n",
    "                    ts = pd.to_datetime(js[\"unix_seconds\"], unit=\"s\", utc=True).tz_convert(TZ_LOCAL)\n",
    "                    return pd.DataFrame({\"da_eur_per_mwh\": vals}, index=ts).sort_index()\n",
    "        except Exception: pass\n",
    "    return pd.DataFrame(columns=[\"da_eur_per_mwh\"])\n",
    "\n",
    "def get_da_series(start_local, end_local):\n",
    "    \"\"\"Get DA series from ENTSO-E; fallback to SMARD/Energy-Charts; normalize to 15-min.\"\"\"\n",
    "    try:\n",
    "        s_br, e_br = to_brussels(start_local), to_brussels(end_local)\n",
    "        da = pc.query_day_ahead_prices(country_code=ZONE_CODE_ENTSOE, start=s_br, end=e_br) # DIRECT CALL\n",
    "        da = da.rename(\"da_eur_per_mwh\").to_frame()\n",
    "    except Exception as e:\n",
    "        print(\"ENTSO-E DA failed:\", repr(e))\n",
    "        da = get_smard_da_qh()\n",
    "        if da.empty:\n",
    "            da = get_energy_charts_da(\"DE\")\n",
    "    idx_qh = make_qh_index(start_local, end_local)\n",
    "    return da.resample(\"15min\").mean().ffill().bfill().reindex(idx_qh)\n",
    "\n",
    "# -------------------------- DA-related features (ENTSO-E forecasts, calendar) --------------------------\n",
    "def get_load_forecast(start_local, end_local):\n",
    "    \"\"\"Robust: returns Series or empty on failure.\"\"\"\n",
    "    try:\n",
    "        s_br, e_br = to_brussels(start_local), to_brussels(end_local)\n",
    "        ser = pc.query_load_forecast(country_code=ZONE_CODE_ENTSOE, start=s_br, end=e_br) # DIRECT CALL\n",
    "        return ser.rename(\"load_fcst_mw\")\n",
    "    except Exception as e:\n",
    "        print(\"ENTSO-E load forecast failed:\", repr(e))\n",
    "        return pd.Series(dtype=float, name=\"load_fcst_mw\")\n",
    "\n",
    "def get_wind_solar_forecast(start_local, end_local):\n",
    "    \"\"\"Robust: returns RES forecast proxy or empty on failure.\"\"\"\n",
    "    try:\n",
    "        s_br, e_br = to_brussels(start_local), to_brussels(end_local)\n",
    "        df = pc.query_wind_and_solar_forecast(country_code=ZONE_CODE_ENTSOE, start=s_br, end=e_br) # DIRECT CALL\n",
    "        if isinstance(df, pd.DataFrame) and not df.empty:\n",
    "            ser = df.select_dtypes(include=[np.number]).sum(axis=1)\n",
    "            ser.name = \"res_fcst_mw\"\n",
    "            return ser\n",
    "        elif isinstance(df, pd.Series):\n",
    "            return df.rename(\"res_fcst_mw\")\n",
    "    except Exception as e:\n",
    "        print(\"ENTSO-E wind/solar forecast failed:\", repr(e))\n",
    "    return pd.Series(dtype=float, name=\"res_fcst_mw\")\n",
    "\n",
    "def build_feature_store(start_local=TRAIN_START, end_local=TRAIN_END):\n",
    "    idx_qh = make_qh_index(start_local, end_local)\n",
    "    da = get_da_series(start_local, end_local)\n",
    "    load_fcst = get_load_forecast(start_local, end_local)\n",
    "    res_fcst  = get_wind_solar_forecast(start_local, end_local)\n",
    "\n",
    "    def to_qh(df_or_ser):\n",
    "        if df_or_ser is None or len(df_or_ser)==0:\n",
    "            if isinstance(df_or_ser, pd.Series) and df_or_ser.name:\n",
    "                df = pd.DataFrame(index=idx_qh, columns=[df_or_ser.name])\n",
    "                return df.astype(float)\n",
    "            return pd.DataFrame(index=idx_qh)\n",
    "        df = df_or_ser if isinstance(df_or_ser, pd.DataFrame) else df_or_ser.to_frame()\n",
    "        return df.resample(\"15min\").mean().ffill().bfill().reindex(idx_qh)\n",
    "\n",
    "    X = to_qh(da)\n",
    "    X = X.join(to_qh(load_fcst))\n",
    "    X = X.join(to_qh(res_fcst))\n",
    "\n",
    "    # Calendar features\n",
    "    X[\"hour\"] = X.index.hour\n",
    "    X[\"quarter\"] = (X.index.minute // 15)\n",
    "    X[\"dow\"] = X.index.dayofweek\n",
    "    X[\"month\"] = X.index.month\n",
    "    X[\"is_weekend\"] = X[\"dow\"].isin([5,6]).astype(int)\n",
    "\n",
    "    # Lags (yesterday & weekly)\n",
    "    for lag_qh in [1, 4, 96, 96*7]:  # 15m, 1h, 1 day, 1 week\n",
    "        X[f\"da_lag_{lag_qh}\"] = X[\"da_eur_per_mwh\"].shift(lag_qh)\n",
    "\n",
    "    # Ensure expected feature columns exist even if forecasts failed\n",
    "    expected_cols = [\"load_fcst_mw\",\"res_fcst_mw\",\"hour\",\"quarter\",\"dow\",\"month\",\"is_weekend\",\n",
    "                     \"da_lag_1\",\"da_lag_4\",\"da_lag_96\",\"da_lag_672\"]\n",
    "    for c in expected_cols:\n",
    "        if c not in X.columns:\n",
    "            X[c] = 0.0\n",
    "\n",
    "    path = f\"{FEATURE_STORE_DIR}/da_features_qh.parquet\"\n",
    "    X.to_parquet(path)\n",
    "    print(f\"✅ DA feature store saved: {path} (shape={X.shape})\")\n",
    "    return X\n",
    "\n",
    "features = build_feature_store()\n",
    "\n",
    "# -------------------------- Train DA model (quantile LightGBM) --------------------------\n",
    "def make_supervised_da(df, target_col=\"da_eur_per_mwh\"):\n",
    "    base = df.dropna(subset=[target_col]).copy()\n",
    "    y = base[target_col]\n",
    "    X = base.drop(columns=[target_col])\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    return X[num_cols], y\n",
    "\n",
    "X_da, y_da = make_supervised_da(features)\n",
    "\n",
    "# Temporal split: last 15% validation\n",
    "# Correction: Convert X_tr and X_va to NumPy arrays (.values)\n",
    "cut = int(0.85 * len(X_da))\n",
    "# X_tr and X_va are now NumPy arrays (no feature names)\n",
    "X_tr, X_va = X_da.iloc[:cut].values, X_da.iloc[cut:].values\n",
    "y_tr, y_va = y_da.iloc[:cut], y_da.iloc[cut:]\n",
    "\n",
    "def train_q_model(Xtr, ytr, alpha):\n",
    "    params = dict(objective=\"quantile\", alpha=alpha, n_estimators=500, learning_rate=0.05, num_leaves=63)\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    pipe = Pipeline([(\"scaler\", StandardScaler(with_mean=False)), (\"lgb\", model)])\n",
    "    pipe.fit(Xtr, ytr)\n",
    "    return pipe\n",
    "\n",
    "mdl_p50 = train_q_model(X_tr, y_tr, 0.5)\n",
    "mdl_p10 = train_q_model(X_tr, y_tr, 0.1)\n",
    "mdl_p90 = train_q_model(X_tr, y_tr, 0.9)\n",
    "\n",
    "def pinball_loss(y_true, y_pred, q):\n",
    "    e = y_true - y_pred\n",
    "    return np.mean(np.maximum(q*e, (q-1)*e))\n",
    "\n",
    "p50_va = mdl_p50.predict(X_va)\n",
    "p10_va = mdl_p10.predict(X_va)\n",
    "p90_va = mdl_p90.predict(X_va)\n",
    "\n",
    "print(\"Validation MAE (p50):\", float(np.mean(np.abs(y_va - p50_va))))\n",
    "print(\"Pinball@10:\", float(pinball_loss(y_va, p10_va, 0.1)),\n",
    "      \"Pinball@50:\", float(pinball_loss(y_va, p50_va, 0.5)),\n",
    "      \"Pinball@90:\", float(pinball_loss(y_va, p90_va, 0.9)))\n",
    "\n",
    "joblib.dump({\"p50\": mdl_p50, \"p10\": mdl_p10, \"p90\": mdl_p90,\n",
    "             \"cols\": X_da.columns.tolist()}, f\"{MODEL_DIR}/da_models_qh.joblib\")\n",
    "print(\"✅ DA models saved.\")\n",
    "\n",
    "# -------------------------- Forecast for a given delivery day --------------------------\n",
    "def build_features_for_day(delivery_local_date: pd.Timestamp):\n",
    "    start_t = delivery_local_date\n",
    "    end_t   = delivery_local_date + pd.Timedelta(days=1)\n",
    "    idx_qh  = make_qh_index(start_t, end_t, TZ_LOCAL)\n",
    "\n",
    "    # Forecast drivers for that delivery day\n",
    "    load_fcst_t = get_load_forecast(start_t, end_t)\n",
    "    res_fcst_t  = get_wind_solar_forecast(start_t, end_t)\n",
    "\n",
    "    # Yesterday DA lags relative to delivery day (align by +1 day)\n",
    "    y_start = start_t - pd.Timedelta(days=1)\n",
    "    y_end   = end_t   - pd.Timedelta(days=1)\n",
    "    da_yday = get_da_series(y_start, y_end)\n",
    "    yday_shifted = da_yday[\"da_eur_per_mwh\"].copy()\n",
    "    yday_shifted.index = yday_shifted.index + pd.Timedelta(days=1)\n",
    "\n",
    "    def to_qh(df_or_ser):\n",
    "        if df_or_ser is None or len(df_or_ser)==0:\n",
    "            if isinstance(df_or_ser, pd.Series) and df_or_ser.name:\n",
    "                df = pd.DataFrame(index=idx_qh, columns=[df_or_ser.name])\n",
    "                return df.astype(float)\n",
    "            return pd.DataFrame(index=idx_qh)\n",
    "        df = df_or_ser if isinstance(df_or_ser, pd.DataFrame) else df_or_ser.to_frame()\n",
    "        return df.resample(\"15min\").mean().ffill().bfill().reindex(idx_qh)\n",
    "\n",
    "    X = pd.DataFrame(index=idx_qh)\n",
    "    X = X.join(to_qh(load_fcst_t).rename(columns={\"load_fcst_mw\":\"load_fcst_mw\"}))\n",
    "    X = X.join(to_qh(res_fcst_t).rename(columns={\"res_fcst_mw\":\"res_fcst_mw\"}))\n",
    "\n",
    "    # Calendar features\n",
    "    X[\"hour\"] = X.index.hour\n",
    "    X[\"quarter\"] = (X.index.minute // 15)\n",
    "    X[\"dow\"] = X.index.dayofweek\n",
    "    X[\"month\"] = X.index.month\n",
    "    X[\"is_weekend\"] = X[\"dow\"].isin([5,6]).astype(int)\n",
    "\n",
    "    # Lags aligned\n",
    "    lag_base = yday_shifted.reindex(idx_qh)\n",
    "    X[\"da_lag_96\"]  = lag_base\n",
    "    X[\"da_lag_1\"]   = lag_base.shift(1)\n",
    "    X[\"da_lag_4\"]   = lag_base.shift(4)\n",
    "    X[\"da_lag_672\"] = lag_base.shift(96*7)\n",
    "\n",
    "  # Ensure all training columns exist\n",
    "    cols = joblib.load(f\"{MODEL_DIR}/da_models_qh.joblib\")[\"cols\"]\n",
    "    for c in cols:\n",
    "        if c not in X.columns:\n",
    "            X[c] = 0.0\n",
    "    # Line 405 correction: Use ffill() and bfill() directly\n",
    "    X = X[cols].ffill().bfill().fillna(0.0) \n",
    "    return X\n",
    "\n",
    "def forecast_for_day(delivery_local_date: pd.Timestamp):\n",
    "    models = joblib.load(f\"{MODEL_DIR}/da_models_qh.joblib\")\n",
    "    X_day = build_features_for_day(delivery_local_date)\n",
    "    p50 = models[\"p50\"].predict(X_day[models[\"cols\"]].values) \n",
    "    p10 = models[\"p10\"].predict(X_day[models[\"cols\"]].values)\n",
    "    p90 = models[\"p90\"].predict(X_day[models[\"cols\"]].values)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"timestamp_local\": X_day.index,\n",
    "        \"price_p10_eur_per_mwh\": np.round(p10, 2),\n",
    "        \"price_p50_eur_per_mwh\": np.round(p50, 2),\n",
    "        \"price_p90_eur_per_mwh\": np.round(p90, 2)\n",
    "    }).set_index(\"timestamp_local\")\n",
    "    return out\n",
    "\n",
    "# -------------------------- Accuracy: previous day actual vs predicted --------------------------\n",
    "def prev_day_actual_vs_pred(prev_local_date: pd.Timestamp):\n",
    "    # Actual DA for prev day (open-data chain)\n",
    "    actual_df = get_da_series(prev_local_date, prev_local_date + pd.Timedelta(days=1))\n",
    "    actual_df = actual_df.rename(columns={\"da_eur_per_mwh\":\"actual_da_eur_per_mwh\"})\n",
    "\n",
    "    # Predicted for prev day\n",
    "    pred_df = forecast_for_day(prev_local_date).rename(columns={\n",
    "        \"price_p50_eur_per_mwh\":\"pred_p50_eur_per_mwh\",\n",
    "        \"price_p10_eur_per_mwh\":\"pred_p10_eur_per_mwh\",\n",
    "        \"price_p90_eur_per_mwh\":\"pred_p90_eur_per_mwh\"\n",
    "    })\n",
    "\n",
    "    # Join and compute errors\n",
    "    comp = pred_df.join(actual_df, how=\"left\")\n",
    "    comp[\"abs_error\"]  = np.abs(comp[\"actual_da_eur_per_mwh\"] - comp[\"pred_p50_eur_per_mwh\"])\n",
    "    comp[\"sq_error\"]   = (comp[\"actual_da_eur_per_mwh\"] - comp[\"pred_p50_eur_per_mwh\"])**2\n",
    "    comp[\"in_band\"]    = ((comp[\"actual_da_eur_per_mwh\"] >= comp[\"pred_p10_eur_per_mwh\"]) &\n",
    "                          (comp[\"actual_da_eur_per_mwh\"] <= comp[\"pred_p90_eur_per_mwh\"])).astype(int)\n",
    "\n",
    "    mae  = float(comp[\"abs_error\"].mean())\n",
    "    rmse = float(np.sqrt(comp[\"sq_error\"].mean()))\n",
    "    coverage = float(100.0 * comp[\"in_band\"].mean())\n",
    "    print(f\"✅ Prev-day metrics → MAE: {mae:.3f} EUR/MWh | RMSE: {rmse:.3f} EUR/MWh | Coverage (p10–p90): {coverage:.2f}%\")\n",
    "    return comp\n",
    "\n",
    "# -------------------------- Two separate CSV outputs --------------------------\n",
    "# 1) Forecast for TOMORROW → CSV\n",
    "forecast_tomorrow_df = forecast_for_day(TOMORROW_LOCAL)\n",
    "csv_forecast_path = f\"{ARTIFACTS_DIR}/DA_forecast_DE-LU_{TOMORROW_LOCAL.date().isoformat()}.csv\"\n",
    "forecast_tomorrow_df.to_csv(csv_forecast_path, index=True)\n",
    "print(f\"✅ Saved DA forecast CSV (tomorrow): {csv_forecast_path} (rows={len(forecast_tomorrow_df)})\")\n",
    "\n",
    "# 2) Previous day actual vs predicted → CSV\n",
    "prev_comp_df = prev_day_actual_vs_pred(PREV_LOCAL_DATE)\n",
    "csv_accuracy_path = f\"{ARTIFACTS_DIR}/DA_prev_day_actual_vs_pred_DE-LU_{PREV_LOCAL_DATE.date().isoformat()}.csv\"\n",
    "prev_comp_df.to_csv(csv_accuracy_path, index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51d1660-22d0-4b91-910f-47d7efac2aca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
